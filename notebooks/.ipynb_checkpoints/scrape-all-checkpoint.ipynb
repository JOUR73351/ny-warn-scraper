{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the import keyword to import pandas, requests, and bs4 modules\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the NY WARN notice url to a variable\n",
    "url = \"https://labor.ny.gov/app/warn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define headers\n",
    "headers = {'accept-encoding': 'deflate', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a get request to the url using the requests library and assign the response to a variable called 'response'\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out status code of response to confirm that your request worked\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the response text using Beautiful Soup's html parser and assign output to a variable called 'soup'\n",
    "# response.text\n",
    "type(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the first table on the page and assign it to a variable called 'table'\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<select id=\"warnYr\" name=\"warnYr\" onchange=\"pageRefresh()\">\n",
       "<option selected=\"\" value=\"2020\">2020</option>\n",
       "<option value=\"2019\">2019</option>\n",
       "<option value=\"2018\">2018</option>\n",
       "<option value=\"2017\">2017</option>\n",
       "<option value=\"2016\">2016</option>\n",
       "<option value=\"2015\">2015</option>\n",
       "<option value=\"2014\">2014</option>\n",
       "<option value=\"2013\">2013</option>\n",
       "<option value=\"2012\">2012</option>\n",
       "</select>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = soup.find(\"select\", id=\"warnYr\")\n",
    "select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [option.text for option in select.find_all(\"option\")]\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array called 'results'\n",
    "results = []\n",
    "#loop through all years\n",
    "for year in years:\n",
    "    # form url for a specific year, eg https://labor.ny.gov/app/warn/default.asp?warnYr=2019\n",
    "    year_url = f'{url}/default.asp?warnYr={year}'\n",
    "    print(url)\n",
    "    print('------------------------------------')\n",
    "    # make get request to url\n",
    "    response = requests.get(year_url, headers=headers)\n",
    "    # scrape the first table on the page and assign it to a variable called 'table'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find(\"table\")\n",
    "    \n",
    "    # get all the rows in the table â€” this is how many WARN notices there were in a specific year\n",
    "    rows = table.find_all(\"tr\")\n",
    "    print(len(rows))\n",
    "    \n",
    "    # grab all rows from the table and assign to a variable called 'rows'\n",
    "    # loop through the rows using a for loop. each row here is a company\n",
    "    for row in rows:\n",
    "        # grab the anchor tag (the link tag) in the row and then grab the href attribute from the tag\n",
    "        a = row.find(\"a\")['href']\n",
    "\n",
    "        # concatenate the root url from above with this href attribute and assign to a variable called 'company_url'\n",
    "        company_url = f'{url}{a}'\n",
    "        #print(company_url)\n",
    "\n",
    "        # make a get request to the company url assign the response to a variable called 'company_response'\n",
    "        company_response = requests.get(company_url, headers=headers)\n",
    "\n",
    "        # parse the response text and assign output to a variable called 'company_soup'\n",
    "        company_soup = BeautifulSoup(company_response.text, 'html.parser')\n",
    "\n",
    "        # grab the first table on the page\n",
    "        company_table = company_soup.find(\"table\")\n",
    "\n",
    "        # unwrap all of the spans\n",
    "\n",
    "        # loop through all of the p tags\n",
    "        paragraphs = company_table.find_all(\"p\")\n",
    "        for p in paragraphs:\n",
    "            # grab all of the values we want\n",
    "            text = p.get_text('\\n')\n",
    "            if 'Date of Notice:' in text:\n",
    "                notice_date = text.split(\":\")[1].strip().split('\\n')[0].strip().replace(',', '').replace(';', '')\n",
    "                #print(notice_date)\n",
    "            elif 'Reason Stated for Filing:' in text:\n",
    "                reason = text.split(\":\")[1].strip()\n",
    "                #print(reason)\n",
    "            elif 'Company:' in text:\n",
    "                split_company = text.split(\"\\n\")\n",
    "                #print(split_company)\n",
    "                company = split_company[1].strip()\n",
    "                address = ''.join(split_company[2:])\n",
    "                 #print(company)\n",
    "                 #print(address)\n",
    "            elif 'County:' in text:\n",
    "                county = f'{text.split(\":\")[1].strip().split(\"|\")[0].strip()} County'\n",
    "                #print(county)\n",
    "            elif 'Phone:' in text:\n",
    "                phone = text.split(\":\")[1].strip()\n",
    "                #print(phone)\n",
    "            elif 'Business Type:' in text:\n",
    "                business_type = text.split(\":\")[1].strip().replace('Restaurants', 'Restaurant')\n",
    "                #print(business_type)\n",
    "            elif 'Number Affected:' in text:\n",
    "                if '-----' in text:\n",
    "                    affected = ''\n",
    "                else:\n",
    "                    affected = text.split(\":\")[1].strip().split(\" \")[0].strip().split('\\n')[0].strip()\n",
    "                #print(affected)\n",
    "            elif 'Total Employees:' in text:\n",
    "                if '-----' in text:\n",
    "                    total_employees = ''\n",
    "                else:\n",
    "                    total_employees = text.split(\":\")[1].strip().split(\" \")[0].strip().replace(',', '')\n",
    "                    #print(total_employees)\n",
    "            elif 'Layoff Date:' in text:\n",
    "                #print(text)\n",
    "                layoff_date = text.split(\":\")[1].strip().split(\" \")[0].strip().split(\" \")[0].strip()\n",
    "                #print(layoff_date)\n",
    "            elif ('Reason for Dislocation:' in text):\n",
    "                dislocation = text.split(\":\")[1].strip()\n",
    "                #print(dislocation)\n",
    "            elif ('Union:' in text):\n",
    "                union = text.split(\":\")[1].strip()\n",
    "                #print(union)\n",
    "            elif ('Classification:' in text):\n",
    "                classification = text.split(\":\")[1].strip()\n",
    "                #print(classification)\n",
    "\n",
    "        # store values in a result object\n",
    "        result = {\n",
    "            'notice_date': notice_date,\n",
    "            'reason': reason,\n",
    "            'company': company,\n",
    "            'address': address,\n",
    "            'county': county,\n",
    "            'phone': phone,\n",
    "            'business_type': business_type,\n",
    "            'affected': affected,\n",
    "            'total_employees': total_employees,\n",
    "            'layoff_date': layoff_date,\n",
    "            'dislocation': dislocation,\n",
    "            'union': union,\n",
    "            'classification': classification\n",
    "         }\n",
    "\n",
    "        # append result object to results\n",
    "        results.append(result)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap results in a dataframe\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3/19/2020', '5/29/2020', '6/1/2020', '3/18/2020', '3/30/2020',\n",
       "       '3/12/2020', '3/27/2020', '4/24/2020', '4/16/2020', '3/20/2020',\n",
       "       '4/3/2020', '6/30/2020', '4/23/2020', '3/16/2020', '3/17/2020',\n",
       "       '4/4/2020', '4/8/2020', '4/22/2020', '3/24/2020', '4/13/2020',\n",
       "       '4/10/2020', 'Furloughs', '3/22/2020', '3/23/2020', 'Separation',\n",
       "       '3/29/2020', '3/25/2020', 'Separations', '32', 'Layoffs',\n",
       "       '7/30/2020', '4/25/2020', '4/1/2020', '7/19/2020', '6/19/2020',\n",
       "       '7/3/2020', '4/20/2020', '3/31/2020', '4/14/2020', '3/28/2020',\n",
       "       '72', 'The', '51', '86', '88', '37', 'Nine', '4/6/2020',\n",
       "       '4/30/2020', '(21)', '(27)', 'March', '30', '18', '14', 'Seven',\n",
       "       '4/2/2020', '3/21/2020', '356', '34', '330', '69', '52', '87',\n",
       "       '81', '-----', '4/7/2020', '48', '3/15/2020', '3/26/2020',\n",
       "       '3/10/2020', '4/12/2020', '362', '5/1/2020', '3/13/2020',\n",
       "       '4/5/2020', '3/19/2020,', '7/12/2020', '4/11/2020', '6/26/2020',\n",
       "       '4/1/2020,', '7/1/2020', '287', 'A', '146', '3/14/2020',\n",
       "       '3/6/2020', 'May', '6/21/2020', '6/24/2020', '2/23/2020',\n",
       "       '3/11/2020', 'Close', '', '6/12/2020', 'June', 'First',\n",
       "       '6/17/2020', 'To', '7/31/2020', '6/3/2020', '6/14/2020',\n",
       "       '5/31/2020', 'December', 'Employee', 'Employment', '2/29/2020',\n",
       "       '5/22/2020', 'April', 'Separtions', 'Employees', '4/27/2020',\n",
       "       \"Macy's\", 'Beginning', '1/12/2020'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 1237\n",
    "df['layoff_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dataframe to a csv\n",
    "df.to_csv('../data/warn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
